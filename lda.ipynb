{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lda.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/catalinux/pphw/blob/master/lda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OAiIuxrLeGm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "9a3af0d4-5b1b-4a00-b740-f8403408af70"
      },
      "source": [
        "!pip install pymc\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import re\n",
        "import pymc\n",
        "import sys\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stemer = SnowballStemmer('english')\n",
        "# prepare for cleaning data\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# print(stop_words)\n",
        "# reading data\n",
        "read = pd.read_csv('data/dummy6.txt', header=None)\n",
        "read.head()\n",
        "\n",
        "\n",
        "# utility method for tokenize document\n",
        "def tokenize(x):\n",
        "    x = x.lower()\n",
        "    s = re.sub(r'[^\\w\\s]', '', x)\n",
        "    words = word_tokenize(s)\n",
        "    tokens = [stemer.stem(w) for w in words if not w in stop_words]\n",
        "    # tokenize = words\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# now we have a data frame with words\n",
        "df = read.applymap(tokenize)[0]\n",
        "\n",
        "df.head()\n",
        "\n",
        "vocab = np.unique(np.concatenate(df.values.flatten()))\n",
        "# store position by word in a dictionary\n",
        "dictionary = dict(zip(vocab, range(0, len(vocab))))\n",
        "print(dictionary)\n",
        "\n",
        "\n",
        "def encode(sentence):\n",
        "    enc = np.zeros(len(sentence))\n",
        "    for i, word in enumerate(sentence):\n",
        "        enc[i] = dictionary[word]\n",
        "    return enc.astype('int')\n",
        "\n",
        "\n",
        "# encode our documents in position in vocab\n",
        "df_encoded = np.empty(len(df), dtype=list)\n",
        "for i, doc in enumerate(df):\n",
        "    df_encoded[i] = encode(doc)\n",
        "\n",
        "print(df_encoded)\n",
        "# peek at a sentece\n",
        "# print(vocab[df_encoded[8].astype(int)])\n",
        "\n",
        "# count of topics\n",
        "K = 2\n",
        "\n",
        "# size of the dictionary\n",
        "V = len(dictionary)\n",
        "\n",
        "print(\"Dictionary size: \", V)\n",
        "# parameter for per topic distributions\n",
        "# phi word distribution for topic\n",
        "# For each topic ùëò, we draw its word distribution, which is denoted as ùúë_ùëò.\n",
        "# As prior for the per-topic word distribution we will use a V-dimensional symmetric Dirichlet distribution\n",
        "beta = np.ones(V) / 10\n",
        "phi = np.empty(K, dtype=object)\n",
        "phi_temp = np.empty(K, dtype=object)\n",
        "for i in range(K):\n",
        "    phi_temp[i] = pymc.Dirichlet(\"temp_phi_%s\" % i, theta=beta, trace=False)\n",
        "    phi[i] = pymc.CompletedDirichlet(\"phi_%s\" % i, phi_temp[i])\n",
        "\n",
        "# size of document collection\n",
        "M = len(df_encoded)\n",
        "alpha = np.ones(K)\n",
        "# theta topic distribution for each document\n",
        "thetas = np.empty(M, dtype=object)\n",
        "thetas_tmp = np.empty(M, dtype=object)\n",
        "z = np.empty(M, dtype=object)\n",
        "w = np.empty(M, dtype=object)\n",
        "\n",
        "for m, document in enumerate(df_encoded):\n",
        "    # we need to create separate variables that will be added to MCMC soup\n",
        "    thetas_tmp[m] = pymc.Dirichlet(\"temp_theta_%s\" % m, theta=alpha, trace=False)\n",
        "    thetas[m] = pymc.CompletedDirichlet(\"theta_%s\" % m, thetas_tmp[m])\n",
        "\n",
        "for m, document in enumerate(df_encoded):\n",
        "    z[m] = np.empty(len(document), dtype=object)\n",
        "    w[m] = np.empty(len(document), dtype=object)\n",
        "    for n, word in enumerate(document):\n",
        "        # z[m][n]  is the topic for the nth word in document m\n",
        "        z[m][n] = pymc.Categorical(\"z_%s_%s\" % (m, n), p=thetas[m])\n",
        "        param = pymc.Lambda(\"phi_z_%s_%s\" % (m, n), lambda t=z[m][n], phi=phi: phi[t])\n",
        "        w[m][n] = pymc.Categorical(\"w_%s_%s\" % (m, n), p=param, value=word, observed=True)\n",
        "\n",
        "model = pymc.Model(\n",
        "    [\n",
        "        pymc.Container(thetas_tmp), pymc.Container(phi_temp),\n",
        "        pymc.Container(phi),\n",
        "        pymc.Container(thetas),\n",
        "        pymc.Container(z),\n",
        "        pymc.Container(w)\n",
        "    ])\n",
        "\n",
        "mc = pymc.MCMC(model)\n",
        "\n",
        "# when using 10K iterations model would not converge well\n",
        "mc.sample(20000, 1000)\n",
        "\n",
        "theta_result = []\n",
        "for i in range(M):\n",
        "    tt = mc.trace('theta_%s' % i)[:].squeeze()\n",
        "    theta_result.append(tt.mean(axis=0))\n",
        "\n",
        "theta_result = np.array(theta_result)\n",
        "\n",
        "phi_result = []\n",
        "for i in range(K):\n",
        "    tt = mc.trace('phi_%s' % i)[:].squeeze()\n",
        "    phi_result.append(tt.mean(axis=0))\n",
        "\n",
        "phi_result = np.array(phi_result)\n",
        "\n",
        "print()\n",
        "print(\"Let's see topics ... \")\n",
        "\n",
        "\n",
        "# see topics\n",
        "def show_topics(top_words):\n",
        "    for i in range(K):\n",
        "        print(\"topic %s:\" % i)\n",
        "        print(vocab[np.argsort(phi_result[i])[::-1][:top_words]])\n",
        "\n",
        "\n",
        "show_topics(2)\n",
        "\n",
        "import math\n",
        "\n",
        "\n",
        "def show_topic_term_score():\n",
        "    term_score = np.empty([K, V])\n",
        "    for k in range(K):\n",
        "        for v in range(V):\n",
        "            term_score[k][v] = phi_result[k][v] * math.log(\n",
        "                phi_result[k][v] / math.pow(np.prod(phi_result[:, v]), 1 / K))\n",
        "    print(term_score)\n",
        "    return term_score\n",
        "\n",
        "\n",
        "z_result = np.empty(M, dtype=object)\n",
        "for m in range(M):\n",
        "    z_result[m] = np.empty(len(df_encoded[m]))\n",
        "    for n in range(len(df_encoded[m])):\n",
        "        z_result[m][n] = np.bincount(mc.trace('z_%s_%s' % (m, n))[:]).argmax()\n",
        "\n",
        "# Question: Can the topic model be used to define a topic based similarity measure between documents? (0.5)\n",
        "# Answer: Yes. We can \"see\" a document as distribution of probabilities over topics. We can use metrics that compare discrete distributions from theta_result\n",
        "# - hellinger (https://en.wikipedia.org/wiki/Hellinger_distance )\n",
        "# - Kullback‚ÄìLeibler divergence (which is not symetric) (https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
        "# - cosine similarity\n",
        "\n",
        "\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "\n",
        "def cos_sim(a, b):\n",
        "    return dot(a, b) / (norm(a) * norm(b))\n",
        "\n",
        "\n",
        "def kl(p, q):\n",
        "    p = np.asarray(p, dtype=np.float)\n",
        "    q = np.asarray(q, dtype=np.float)\n",
        "\n",
        "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
        "\n",
        "\n",
        "def hellinger(p, q):\n",
        "    return np.sqrt(np.sum((np.sqrt(p) - np.sqrt(q)) ** 2)) / np.sqrt(2)\n",
        "\n",
        "\n",
        "theta_result = theta_result.squeeze()\n",
        "print(\"Similarity Cosine, 1 - very similar\")\n",
        "similiarity = np.empty([M, M])\n",
        "for i in range(M):\n",
        "    for j in range(M):\n",
        "        similiarity[i, j] = cos_sim(theta_result[i], theta_result[j])\n",
        "print(similiarity.round(2))\n",
        "\n",
        "print(\"Similarity hellinger, 1 - very similar\")\n",
        "similiarity = np.empty([M, M])\n",
        "for i in range(M):\n",
        "    for j in range(M):\n",
        "        similiarity[i, j] = hellinger(theta_result[i], theta_result[j])\n",
        "print(similiarity.round(2))\n",
        "\n",
        "print(\"Similarity kl,  0 indicates that the two distributions in question are identical\")\n",
        "similiarity = np.empty([M, M])\n",
        "for i in range(M):\n",
        "    for j in range(M):\n",
        "        similiarity[i, j] = kl(theta_result[i], theta_result[j])\n",
        "print(similiarity.round(2))\n",
        "\n",
        "# Question: What about a new document? How can topics be assigned to it?\n",
        "\n",
        "\n",
        "# Correlated topic model\n",
        "\n",
        "# In LDA, we assume two things:\n",
        "#  - Topics in a document are independent\n",
        "#  - Distribution of words in a topic is stationary\n",
        "\n",
        "# According to \"A CORRELATED TOPIC MODEL OF SCIENCE\" article\n",
        "\n",
        "# The correlated topic model. The correlated topic model (CTM) is a\n",
        "# hierarchical model of document collections. The CTM models the words of\n",
        "# each document from a mixture model. The mixture components are shared\n",
        "# by all documents in the collection; the mixture proportions are document specific random variables.\n",
        "# The CTM allows each document to exhibit multiple topics with different proportions.\n",
        "#\n",
        "#\n",
        "# sys.exit()\n",
        "#\n",
        "# miu_lower = -0.01\n",
        "# miu_upper = 0.01\n",
        "# miu = np.empty(M, dtype=object)\n",
        "# for i in range(V):\n",
        "#     miu[i] = np.empty(M, dtype=object)\n",
        "#     for j in range(M):\n",
        "#         miu[i][j] = pymc.Uniform('miu_%s_%s' % (i, j), lower=miu_lower, upper=miu_upper)\n",
        "#\n",
        "# tau = pymc.Container([pymc.Wishart('tau_{0}'.format(d), n=K + 1, Tau=np.eye(K))\n",
        "#                       for d in range(M)])\n",
        "#\n",
        "# eta = pymc.Container([pymc.MvNormal('eta_{0}'.format(d), mu=miu[d], tau=tau[d])\n",
        "#                       for d in range(M)])\n",
        "#\n",
        "# # for m, document in enumerate(df_encoded):\n",
        "# #     # we need to create separate variables that will be added to MCMC soup\n",
        "# #     thetas_tmp[m] = pymc.Dirichlet(\"temp_theta_%s\" % m, theta=alpha)\n",
        "# #     thetas[m] = pymc.CompletedDirichlet(\"theta_%s\" % m, thetas_tmp[m])\n",
        "#\n",
        "#\n",
        "# for m, document in enumerate(df_encoded):\n",
        "#     z[m] = np.empty(len(document), dtype=object)\n",
        "#     w[m] = np.empty(len(document), dtype=object)\n",
        "#     for n, word in enumerate(document):\n",
        "#         # z[m][n]  is the topic for the nth word in document m\n",
        "#         param_z = pymc.Lambda(\"p_z_%s_%s\" % (m, n), lambda e=eta[m]: np.exp(e) / np.sum(np.exp(e)))\n",
        "#         z[m][n] = pymc.Categorical(\"z_%s_%s\" % (m, n), p=param_z)\n",
        "#         param = pymc.Lambda(\"phi_z_%s_%s\" % (m, n), lambda t=z[m][n], phi=phi: phi[t])\n",
        "#         w[m][n] = pymc.Categorical(\"w_%s_%s\" % (m, n), p=param, value=word, observed=True)\n",
        "#\n",
        "#\n",
        "# model = pymc.Model([pymc.Container(phi), pymc.Container(miu), pymc.Container(tau), pymc.Container(eta), pymc.Container(z), pymc.Container(w)])\n",
        "#\n",
        "# mcmc = pymc.MCMC(model)\n",
        "# mcmc.sample(20000, 5000, 1)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymc in /usr/local/lib/python3.6/dist-packages (2.3.6)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "{'almond': 0, 'breakfast': 1, 'butter': 2, 'cat': 3, 'dog': 4, 'eat': 5, 'enemi': 6, 'feed': 7, 'got': 8, 'like': 9, 'littl': 10, 'mortal': 11, 'mustnt': 12, 'neighbor': 13, 'peanut': 14, 'sandwich': 15, 'walnut': 16}\n",
            "[array([14,  2, 15,  1]) array([ 9,  5,  0, 14, 16])\n",
            " array([13,  8, 10,  4]) array([ 3,  4, 11,  6]) array([12,  7, 14,  4])]\n",
            "Dictionary size:  17\n",
            " [-----------------100%-----------------] 20000 of 20000 complete in 82.1 sec\n",
            "Let's see topics ... \n",
            "topic 0:\n",
            "['butter' 'enemi']\n",
            "topic 1:\n",
            "['walnut' 'dog']\n",
            "Similarity Cosine, 1 - very similar\n",
            "[[1.   0.93 0.95 0.96 1.  ]\n",
            " [0.93 1.   1.   1.   0.93]\n",
            " [0.95 1.   1.   1.   0.95]\n",
            " [0.96 1.   1.   1.   0.96]\n",
            " [1.   0.93 0.95 0.96 1.  ]]\n",
            "Similarity hellinger, 1 - very similar\n",
            "[[0.   0.14 0.12 0.11 0.  ]\n",
            " [0.14 0.   0.02 0.03 0.14]\n",
            " [0.12 0.02 0.   0.01 0.12]\n",
            " [0.11 0.03 0.01 0.   0.11]\n",
            " [0.   0.14 0.12 0.11 0.  ]]\n",
            "Similarity kl,  0 indicates that the two distributions in question are identical\n",
            "[[0.   0.08 0.06 0.05 0.  ]\n",
            " [0.08 0.   0.   0.   0.08]\n",
            " [0.06 0.   0.   0.   0.06]\n",
            " [0.05 0.   0.   0.   0.05]\n",
            " [0.   0.08 0.06 0.05 0.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUzoFk05Mylz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QNn1c40My1A",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOzMXN2NMUgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}